{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"drive-path.txt\", 'r') as f:\n",
    "    download_path = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "#https://stackoverflow.com/a/53877507/1558946\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_data(url):\n",
    "    print(f\"{url} 다운로드 중 ...\")\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        zip_path, _ = urllib.request.urlretrieve(url, reporthook=t.update_to)\n",
    "\n",
    "    print(\"압축을 푸는 중 ...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        for name in tqdm(iterable=f.namelist(), total=len(f.namelist())):\n",
    "            # f.extract(member=name, path=\"data_dir\")\n",
    "            f.extract(member=name, path=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://images.cocodataset.org/zips/train2014.zip 다운로드 중 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train2014.zip: 13.5GB [11:02:06, 340kB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축을 푸는 중 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82784/82784 [05:36<00:00, 245.85it/s]\n"
     ]
    }
   ],
   "source": [
    "download_data(\"http://images.cocodataset.org/zips/train2014.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://images.cocodataset.org/zips/val2014.zip 다운로드 중 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val2014.zip: 6.65GB [5:18:23, 348kB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축을 푸는 중 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40505/40505 [02:15<00:00, 299.69it/s]\n"
     ]
    }
   ],
   "source": [
    "download_data(\"http://images.cocodataset.org/zips/val2014.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotoolsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for pycocotools from https://files.pythonhosted.org/packages/24/b2/ef28a34cf6ca50b6b2f7ad81e5837ed45c252ffef22f5a704b94141ea842/pycocotools-2.0.7-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pycocotools-2.0.7-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\wlghks\\appdata\\roaming\\python\\python311\\site-packages (from pycocotools) (3.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from pycocotools) (1.25.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wlghks\\anaconda3\\envs\\ml_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Downloading pycocotools-2.0.7-cp311-cp311-win_amd64.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.8 kB ? eta -:--:--\n",
      "   -------------------------------------- - 81.9/85.8 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 85.8/85.8 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.7\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    "\n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, token in enumerate(tokens):\n",
    "        vocab.add_token(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
    "vocab_path = './data_dir/vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
